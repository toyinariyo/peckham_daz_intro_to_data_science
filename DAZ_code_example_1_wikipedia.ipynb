{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZOlIW07E38oR"
      },
      "outputs": [],
      "source": [
        "#import required libraries\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import random\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjuUeU6l38oS"
      },
      "outputs": [],
      "source": [
        "#check we can get the data from the page\n",
        "\n",
        "#here we use 'query' for the end of the url, this allows us to quickly change it\n",
        "query = 'AFC_Wimbledon'\n",
        "\n",
        "url = 'https://en.wikipedia.org/wiki/' + query\n",
        "response = requests.get(url)\n",
        "bs_html = BeautifulSoup(response.text, features=\"html.parser\")\n",
        "\n",
        "#this grabs us the html of the entire page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fgasRI04io1"
      },
      "outputs": [],
      "source": [
        "# we can preview the html contents\n",
        "print(bs_html.prettify())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pflwu_q-38oT"
      },
      "outputs": [],
      "source": [
        "#this will check if the request was sucsessful. we want it to be 200, or at least start with a 2... anything else is a problem.\n",
        "\n",
        "print(response.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ovc3n6p38oT"
      },
      "outputs": [],
      "source": [
        "#now I want to find only the link on this page\n",
        "#first we create an array for the links\n",
        "\n",
        "links = []\n",
        "\n",
        "#we are looking for all of the <a> anchor tags.\n",
        "# we do this with a for loop, we use 'try' and 'except' as some of the anchors may not have an 'href'. we ignore these otherwise it could cause an error.\n",
        "\n",
        "for a in bs_html.find_all(\"a\"):\n",
        "    try:\n",
        "        links.append(a[\"href\"])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "    #then another for loop to cycle though the array and print each link\n",
        "for link in links:\n",
        "    print(link)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYwXwfW438oU"
      },
      "outputs": [],
      "source": [
        "#many of the links are from outside wikipedia. in this case we only want internal links\n",
        "\n",
        "#we can then filter the array to only include links starting with /wiki/. so only internal links will show.\n",
        "\n",
        "filtered = []\n",
        "\n",
        "for link in links:\n",
        "  if link.startswith('/wiki/'):\n",
        "    filtered.append(link)\n",
        "\n",
        "for f in filtered:\n",
        "    print(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7uh3MWw38oU"
      },
      "outputs": [],
      "source": [
        "#there are still a lot of links to stuff we dont want eg. pictures, help files ect. We can use ignore to filter them out.\n",
        "\n",
        "ignores = ['png', 'jpg', 'jpeg', 'isbn', 'svg', 'identifier', \\\n",
        "           'File', 'Special', 'Template', 'Mailto', 'Portal', \\\n",
        "           'Help', 'Category', 'Talk', 'Wikipedia', 'Main_Page']\n",
        "\n",
        "filtered = []\n",
        "\n",
        "#this line states only links that are to wiki pages are valid\n",
        "for link in links:\n",
        "    if link.startswith('/wiki/'):\n",
        "        valid = True\n",
        "\n",
        "        # this line then makes all our ingnored links invalid\n",
        "        for ignore in ignores:\n",
        "            if ignore in link:\n",
        "                valid = False\n",
        "                break\n",
        "\n",
        "        # if the link is valid we then add it to our 'filtered' array\n",
        "        if valid:\n",
        "            filtered.append(link)\n",
        "\n",
        "for f in filtered:\n",
        "    print(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQyT8iQu38oU"
      },
      "outputs": [],
      "source": [
        "# get the response in the form of html\n",
        "wikiurl=\"https://en.wikipedia.org/wiki/AFC_Wimbledon\"\n",
        "\n",
        "# check the request was sucsessful (code 200)\n",
        "response=requests.get(wikiurl)\n",
        "print(response.status_code)\n",
        "\n",
        "# parse data from the html into a beautifulsoup object\n",
        "bs_html = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# here we find any element with the table tag, there are some of these we dont want on this page.\n",
        "# So we specify only tables using the \"wikitable\" class\n",
        "\n",
        "tabledata=bs_html.find('table',{'class':\"wikitable\"})\n",
        "\n",
        "#read the table data\n",
        "df=pd.read_html(str(tabledata))\n",
        "\n",
        "# convert list to pandas dataframe\n",
        "df=pd.DataFrame(df[0])\n",
        "print(df.head())\n",
        "\n",
        "#write the data to a .csv file\n",
        "df.to_csv('team_info.csv', sep='\\t', encoding='utf-8')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
